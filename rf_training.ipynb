{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda22c04-aa89-4dfb-9af7-d301ff2b36a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "import os\n",
    "\n",
    "\n",
    "import time\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from boruta import BorutaPy\n",
    "from copy import deepcopy\n",
    "import shap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a8904-370a-4888-b3cb-2b025f994cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Combine results\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "sp_rarefied_name_list = ['San','Rub','Rec','Pro','Mex','Maz','Lon','Lec','Ind','Ger','Dim']\n",
    "\n",
    "buffer_name_list = ['b_05']\n",
    "\n",
    "rarefied_name_list = [''] #using the 5km rarefied data. Leave this blank if you don't have special notation on your file name for the rarefaction.\n",
    "\n",
    "for rarefied_name in rarefied_name_list:\n",
    "    for buffer_name in buffer_name_list:\n",
    "        for bug_name in sp_rarefied_name_list:\n",
    "\n",
    "            grid_name = '5' # We are using 0.05Ëš grid\n",
    "            \n",
    "            name_for_saving = bug_name+rarefied_name+'_g'+grid_name+'_'+buffer_name \n",
    "            \"\"\"\n",
    "            modified\n",
    "            \"\"\"\n",
    "            print(name_for_saving)\n",
    "\n",
    "            buffer_path = '/Users/huangliting/Desktop/kissing_bugs_stats/buffers/'+bug_name+rarefied_name+'_g0'+grid_name+'_'+buffer_name+'.csv'\n",
    "            buffer_select = pd.read_csv(buffer_path)\n",
    "            buffer_select.drop(columns=['left', 'top', 'right', 'bottom'], inplace=True)\n",
    "\n",
    "\n",
    "            presence_path = '/Users/huangliting/Desktop/kissing_bugs_stats/presence_point/'+bug_name+rarefied_name+'_g'+grid_name+'.csv'\n",
    "            presence_select = pd.read_csv(presence_path)\n",
    "            presence_select.drop(columns=['left', 'top', 'right', 'bottom'], inplace=True)\n",
    "\n",
    "\n",
    "            zonal_path = '/Users/huangliting/Desktop/kissing_bugs_stats/zonal_stats_csv/'+grid_name+'km.csv'\n",
    "            zonal_select = pd.read_csv(zonal_path)\n",
    "            zonal_select.drop(columns=['Unnamed: 0','left', 'top', 'right', 'bottom','forest_grassland'], inplace=True)\n",
    "\n",
    "            raw_df = pd.merge(pd.merge(buffer_select, presence_select, on='id'),zonal_select, on='id')\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            select presence data\n",
    "            \"\"\"\n",
    "            df0 = raw_df.copy()\n",
    "\n",
    "            species_data = df0.loc[df0['count'] ==1].copy()\n",
    "            species_data= species_data.loc[~species_data['NFFD'].isnull()].copy()\n",
    "\n",
    "\n",
    "\n",
    "            species_data = species_data.loc[~species_data['water'].isnull()].copy()\n",
    "            species_data = species_data.loc[~species_data['cropland'].isnull()].copy()\n",
    "            species_data = species_data.loc[~species_data['permanent_snow_ice'].isnull()].copy()\n",
    "            species_data = species_data.loc[~species_data['barren'].isnull()].copy()\n",
    "            species_data = species_data.loc[~species_data['urban'].isnull()].copy()\n",
    "            species_data = species_data.loc[~species_data['grassland'].isnull()].copy()\n",
    "            species_data = species_data.loc[~species_data['forest'].isnull()].copy()\n",
    "\n",
    "            species_data['presence']=1\n",
    "\n",
    "            species_data.drop(columns=['id','count', buffer_name], inplace=True)\n",
    "\n",
    "            number_of_presence = species_data.shape[0]\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            select north america\n",
    "            \"\"\"\n",
    "            df_na = raw_df.copy()\n",
    "            background_data= df_na.loc[~df_na['NFFD'].isnull()].copy()\n",
    "\n",
    "\n",
    "            background_data = background_data.loc[~background_data['water'].isnull()].copy()\n",
    "            background_data = background_data.loc[~background_data['cropland'].isnull()].copy()\n",
    "            background_data = background_data.loc[~background_data['permanent_snow_ice'].isnull()].copy()\n",
    "            background_data = background_data.loc[~background_data['barren'].isnull()].copy()\n",
    "            background_data = background_data.loc[~background_data['urban'].isnull()].copy()\n",
    "            background_data = background_data.loc[~background_data['grassland'].isnull()].copy()\n",
    "            background_data = background_data.loc[~background_data['forest'].isnull()].copy()\n",
    "\n",
    "\n",
    "            testing_group_raw = background_data.copy()\n",
    "            id_list = testing_group_raw['id']\n",
    "            testing_group_raw.drop(['id','count', buffer_name], axis=1,inplace=True)\n",
    "            final_testing_group = testing_group_raw.copy()\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            select absence pool\n",
    "            \"\"\"\n",
    "            buffer_selection_raw = background_data.copy()\n",
    "            buffer_background = buffer_selection_raw.loc[buffer_selection_raw[buffer_name].isnull()].copy()\n",
    "            buffer_background['presence']=0\n",
    "            buffer_background.drop(['id','count', buffer_name], axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            define scoring\n",
    "            \"\"\"\n",
    "\n",
    "            def tss_scorer(clf, X, y):\n",
    "                y_pred = clf.predict(X)\n",
    "                cm = confusion_matrix(y, y_pred,labels=[0,1])\n",
    "                tss = (cm[0, 0]/(cm[0, 0]+cm[1, 0]))+(cm[1, 1]/(cm[1, 1]+cm[0, 1]))-1\n",
    "                return tss\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            modify here\n",
    "            \"\"\"\n",
    "\n",
    "            # Disable the printing\n",
    "            def blockPrint():\n",
    "                sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "            # Restore\n",
    "            def enablePrint():\n",
    "                sys.stdout = sys.__stdout__\n",
    "\n",
    "            num_runs = 50 #total number of iteration\n",
    "\n",
    "            count_num = 0 #the current number of iteration\n",
    "\n",
    "            count_average = 0 #for storing average cross-validation score\n",
    "\n",
    "            array_list = np.array([]) #record permutation importance, importance\n",
    "            variance_list = np.array([]) #record permutation importance, variance\n",
    "\n",
    "            giniarray_list = np.array([]) #record gini importance, importance\n",
    "            ginivariance_list = np.array([]) #record gini importance, variance\n",
    "\n",
    "            predict_list = np.array([]) #record prediction of north america\n",
    "\n",
    "            alldata_list = np.array([]) #record all input data of north america\n",
    "\n",
    "            fpr_list = np.array([]) #record roc of north america\n",
    "\n",
    "            tpr_list = np.array([]) #record roc of north america\n",
    "\n",
    "            roc_auc_count = 0 #record auc of north america\n",
    "\n",
    "            X_list = np.array([])\n",
    "            y_list = np.array([]) #store the data used in each run\n",
    "\n",
    "            boruta_rank = np.array([]) #store boruta rank in each run\n",
    "\n",
    "            shape_values_list0 = np.array([])\n",
    "            shape_values_list1 = np.array([])#store shapevalues\n",
    "            x_train_list = np.array([])\n",
    "\n",
    "            \"\"\"change_name\"\"\"\n",
    "            buffer_used = buffer_background.copy() #the buffer to select psudo-absence points\n",
    "\n",
    "            size_group = species_data.shape[0]#number of presence data points\n",
    "\n",
    "\n",
    "            x_labels = np.delete(species_data.keys(), np.argwhere(species_data.keys() == 'presence'))\n",
    "            variable_length = len(x_labels) # for making sure the variables are arranged in the same order later \n",
    "\n",
    "            #create a directory for storing the trees\n",
    "            save_tree_dir = '/Users/huangliting/Desktop/kissing_bugs_stats/trees/'+name_for_saving \n",
    "\n",
    "\n",
    "            if not os.path.exists(save_tree_dir):\n",
    "                os.makedirs(save_tree_dir)\n",
    "\n",
    "\n",
    "            x_train_length = 0\n",
    "\n",
    "            while count_num < num_runs:\n",
    "\n",
    "                buffer_thisround = buffer_used.copy()\n",
    "\n",
    "                background_data = buffer_thisround.sample(n=size_group)\n",
    "                species_thisround = species_data.copy()\n",
    "                combine_list = [species_thisround,background_data]\n",
    "\n",
    "                combined_df = pd.concat(combine_list)\n",
    "\n",
    "\n",
    "\n",
    "                # Get the y values\n",
    "                y=combined_df['presence']\n",
    "                y=y.astype('int')\n",
    "\n",
    "                # Get the x values\n",
    "                X = combined_df.copy()\n",
    "                X.drop(['presence'], axis=1,inplace=True)\n",
    "                X.astype('float')\n",
    "\n",
    "                X_list = np.append(X_list, X)\n",
    "                y_list = np.append(y_list, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Split dataset into training set and test set, using in hyper variable tuning and permutation importance\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% tes\n",
    "\n",
    "                # Define the Random Forest Classifier\n",
    "                rfclassifier = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "                \"\"\"\n",
    "                shuffle split then calculate the cross_validation TSS score\n",
    "                \"\"\"\n",
    "                cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0) # 70% training set\n",
    "\n",
    "                best_cross_score = np.mean(cross_val_score(rfclassifier, X, y, cv=cv,scoring=tss_scorer))\n",
    "                count_average+=best_cross_score\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                Train the RF model\n",
    "                \"\"\"\n",
    "                n_repeats=100\n",
    "\n",
    "                print(\"length_ytest: \"+str(len(y_test)))\n",
    "                \n",
    "                rfclassifier.fit(X_train,y_train)\n",
    "                \n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                mean-decrease in gini impurity importance\n",
    "                \"\"\" \n",
    "\n",
    "                gini_importances = rfclassifier.feature_importances_\n",
    "                gini_std = np.std([tree.feature_importances_ for tree in rfclassifier.estimators_], axis=0)\n",
    "\n",
    "                ginithis_importance_score = pd.Series(gini_importances, index=x_labels)\n",
    "                giniarray_list = np.append(giniarray_list, np.array(ginithis_importance_score))\n",
    "                ginithis_importance_variance = np.array(gini_std)\n",
    "                ginivariance_list = np.append(ginivariance_list,ginithis_importance_variance)\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                Boruta\n",
    "                \"\"\"\n",
    "                blockPrint()\n",
    "                # define Boruta feature selection method\n",
    "                this_random_forest = deepcopy(rfclassifier)\n",
    "                feat_selector = BorutaPy(this_random_forest, n_estimators='auto', verbose=2, random_state=42)\n",
    "\n",
    "                X_train_np = np.array(X_train)\n",
    "                y_train_np = np.array(y_train)\n",
    "\n",
    "\n",
    "                # find all relevant features\n",
    "                feat_selector.fit(X_train_np, y_train_np)\n",
    "\n",
    "                # call transform() on X to filter it down to selected features\n",
    "                X_filtered = feat_selector.transform(X_train_np)\n",
    "\n",
    "                feature_ranks = np.array(list(zip(x_labels,feat_selector.ranking_, feat_selector.support_)))\n",
    "\n",
    "                # iterate through and print out the results\n",
    "\n",
    "                for feat in feature_ranks:\n",
    "                    print('Feature: {:<25} Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    "\n",
    "\n",
    "                boruta_rank_array = feature_ranks[:,1].astype('float')\n",
    "                boruta_rank = np.append(boruta_rank,boruta_rank_array)\n",
    "\n",
    "\n",
    "                enablePrint()\n",
    "\n",
    "                \"\"\"\n",
    "                Shapley value\n",
    "                \"\"\"\n",
    "                shap_random_forest = deepcopy(rfclassifier)\n",
    "                shap_random_forest1 = deepcopy(rfclassifier)\n",
    "                shap_values = shap.TreeExplainer(shap_random_forest).shap_values(X_train)\n",
    "\n",
    "                shape_values_list0 = np.append(shape_values_list0, shap_values[0])\n",
    "                shape_values_list1 = np.append(shape_values_list1, shap_values[1])\n",
    "                \n",
    "                x_train_length = X_train.shape[0]\n",
    "                x_train_list = np.append(x_train_list, np.array(X_train))\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                Record the tree\n",
    "                \"\"\"\n",
    "                tree_filename = save_tree_dir+'/'+name_for_saving+'_'+str(count_num)+'iters.sav'\n",
    "                joblib.dump(rfclassifier, tree_filename)\n",
    "\n",
    "                \"\"\"\n",
    "                Test on the whole North America\n",
    "                \"\"\"\n",
    "                final_testing_thisround = final_testing_group.copy()\n",
    "                final_prediction=rfclassifier.predict(final_testing_thisround)\n",
    "                predict_list = np.append(predict_list, final_prediction)\n",
    "\n",
    "                \"\"\"\n",
    "                Compute ROC and AUC\n",
    "                \"\"\"\n",
    "                # Compute ROC curve and ROC area for each class\n",
    "                y_score = rfclassifier.predict_proba(X_test)[:,1]\n",
    "\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "                roc_auc_count = roc_auc_count+ roc_auc\n",
    "\n",
    "                n = 1000\n",
    "                x_interp = np.linspace(0,1,n)\n",
    "                fpr_list = np.append(fpr_list, x_interp)\n",
    "                y_interp = np.interp(x_interp,fpr, tpr)\n",
    "                tpr_list = np.append(tpr_list, y_interp)\n",
    "\n",
    "\n",
    "                count_num+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #save everything in a dictionary\n",
    "\n",
    "            #cross validation\n",
    "            cross_val_score_ave = count_average/num_runs\n",
    "\n",
    "            #gini importance\n",
    "\n",
    "            giniarray_list1 = giniarray_list.reshape(num_runs, variable_length)\n",
    "            giniarray_list1 = np.mean(giniarray_list1, axis=0)\n",
    "\n",
    "            ginivariance_list0 = ginivariance_list.reshape(num_runs, variable_length)\n",
    "            ginivariance_list0 = np.mean(ginivariance_list0,axis=0)\n",
    "\n",
    "            #ROC\n",
    "            #fpr\n",
    "            fpr_list1 = fpr_list.reshape(num_runs, 1000)\n",
    "            #tpr\n",
    "            tpr_list1 = tpr_list.reshape(num_runs, 1000)\n",
    "            \n",
    "            #auc\n",
    "            roc_auc_count_ave = roc_auc_count/num_runs\n",
    "\n",
    "\n",
    "            #boruta\n",
    "            boruta_rank1 = boruta_rank.reshape(num_runs, variable_length)\n",
    "            boruta_rank1 = np.mean(boruta_rank1,axis=0)\n",
    "\n",
    "            #shapely\n",
    "\n",
    "            shape_values_list00 = np.mean(shape_values_list0.reshape(x_train_length*num_runs,variable_length), axis=0)\n",
    "            shape_values_list11 = np.mean(shape_values_list1.reshape(x_train_length*num_runs,variable_length), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            x_train_list001 = x_train_list.reshape(x_train_length*num_runs,variable_length)\n",
    "            x_train_list1 = pd.DataFrame(x_train_list001, columns = x_labels)\n",
    "\n",
    "            dict_save = {'cross_validation_score' : cross_val_score_ave, \n",
    "                    'x_label' : x_labels, \n",
    "                    'X_list':X_list,\n",
    "                    'y_list':y_list,\n",
    "                    'number_of_presence':number_of_presence,\n",
    "                    'gini_importance_score' : giniarray_list1,\n",
    "                    'gini_importance_variance' : ginivariance_list0,\n",
    "                    'boruta_rank':boruta_rank1,\n",
    "                    'auc':roc_auc_count_ave,\n",
    "                    'fpr':fpr_list1,\n",
    "                    'tpr':tpr_list1,\n",
    "                    'x_train_list1':x_train_list1,\n",
    "                    'shape_values_list0':shape_values_list00,\n",
    "                    'shape_values_list1':shape_values_list11}\n",
    "            \n",
    "            dictionary_path = save_tree_dir + '/'+name_for_saving+'_dictionary_'+str(num_runs)+'iters.pickle'\n",
    "            with open(dictionary_path, 'wb') as handle:\n",
    "                pickle.dump(dict_save, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n",
    "            with open(dictionary_path, 'rb') as handle:\n",
    "                dict_read = pickle.load(handle)\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            ROC\n",
    "            \"\"\"\n",
    "\n",
    "            fpr_plot = dict_read['fpr']\n",
    "            tpr_plot = dict_read['tpr']\n",
    "            #print(tpr_plot)\n",
    "            plt.figure(figsize=(10,10))\n",
    "            lw = 2\n",
    "\n",
    "            for i in range(len(tpr_plot)):\n",
    "                plt.plot(fpr_plot[0],tpr_plot[i])\n",
    "\n",
    "            plt.xlim([-0.05, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel(\"False Positive Rate\",fontsize = 15)\n",
    "            plt.ylabel(\"True Positive Rate\",fontsize = 15)\n",
    "            plt.tick_params(axis='x', labelsize= 15 )\n",
    "            plt.tick_params(axis='y', labelsize= 15 )\n",
    "            plt.title('ROC '+ name_for_saving+' ('+str(num_runs)+' iters)',fontsize = 20)\n",
    "\n",
    "            #create a directory for storing the ROC\n",
    "            ROC_dir = '/Users/huangliting/Desktop/kissing_bugs_stats/ROC/'+name_for_saving\n",
    "\n",
    "            #print(ROC_dir)\n",
    "\n",
    "            if not os.path.exists(ROC_dir):\n",
    "                os.makedirs(ROC_dir)\n",
    "\n",
    "\n",
    "            store_graph_name = ROC_dir+'/'+name_for_saving+str(num_runs)+'iters.png'\n",
    "            #print(store_graph_name)\n",
    "            plt.savefig(store_graph_name)\n",
    "\n",
    "\n",
    "            print(\"cross_validation: \"+str(cross_val_score_ave))\n",
    "            print(\"auc: \"+str(roc_auc_count_ave))\n",
    "            plt.clf()\n",
    "\n",
    "            \"\"\"\n",
    "            gini\n",
    "            \"\"\"\n",
    "\n",
    "            giniforest_importance1 = pd.Series(giniarray_list1, index=x_labels)\n",
    "\n",
    "            #print(forest_importance1)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "            giniforest_importance1.plot.bar(yerr=ginivariance_list0, ax=ax, fontsize=30)\n",
    "            ax.set_title(\"mean decrease gini impurity importances \" + name_for_saving+' ('+str(num_runs)+' iters)',fontsize=30)\n",
    "            ax.set_ylabel(\"Mean decrease in impurity\",fontsize=30)\n",
    "            plt.grid()\n",
    "            fig.tight_layout()\n",
    "\n",
    "            #create a directory for storing the trees\n",
    "            gini_dir = '/Users/huangliting/Desktop/kissing_bugs_stats/gini_importance/'+name_for_saving\n",
    "\n",
    "            #print(gini_dir)\n",
    "\n",
    "            if not os.path.exists(gini_dir):\n",
    "                os.makedirs(gini_dir)\n",
    "\n",
    "\n",
    "            store_graph_name = gini_dir+'/'+name_for_saving+'_'+str(num_runs)+'iters.png'\n",
    "            #print(store_graph_name)\n",
    "            plt.savefig(store_graph_name)\n",
    "            plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            boruta graph\n",
    "            \"\"\"\n",
    "            boruta_importance1 = pd.Series(boruta_rank1, index=x_labels)\n",
    "            fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "            boruta_importance1.plot.bar(ax=ax, fontsize=30)\n",
    "            ax.set_title(\"boruta importances \" + name_for_saving+' ('+str(num_runs)+' iters)',fontsize=30)\n",
    "            ax.set_ylabel(\"Mean rank\",fontsize=30)\n",
    "            plt.grid()\n",
    "            fig.tight_layout()\n",
    "\n",
    "\n",
    "            #create a directory for storing the trees\n",
    "            boruta_dir = '/Users/huangliting/Desktop/kissing_bugs_stats/boruta_importance/'+name_for_saving\n",
    "\n",
    "\n",
    "            if not os.path.exists(boruta_dir):\n",
    "                os.makedirs(boruta_dir)\n",
    "\n",
    "\n",
    "            store_graph_name = boruta_dir+'/'+name_for_saving+'_'+str(num_runs)+'iters.png'\n",
    "            #print(store_graph_name)\n",
    "            plt.savefig(boruta_dir+'/'+name_for_saving+'_'+str(num_runs)+'iters.png')\n",
    "            plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            shapely graph\n",
    "            \"\"\"\n",
    "            fig, ax = plt.subplots(figsize=(60,20))\n",
    "            x_shaply_value = np.arange(len(x_labels))  # the label locations\n",
    "            width = 0.35  # the width of the bars\n",
    "\n",
    "\n",
    "            rects1 = ax.bar(x_shaply_value - width/2, np.abs(shape_values_list00), width, label='absence')\n",
    "            rects2 = ax.bar(x_shaply_value + width/2, np.abs(shape_values_list11), width, label='presence')\n",
    "\n",
    "            # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "            ax.set_ylabel('Scores',fontsize=40,)\n",
    "            ax.set_title('Scores by group and presence-absence')\n",
    "            ax.set_xticks(x_shaply_value, x_labels,fontsize=40,rotation='vertical')\n",
    "            ax.legend(fontsize=40)\n",
    "\n",
    "            # ax.bar_label(rects1, padding=3)\n",
    "            # ax.bar_label(rects2, padding=3)\n",
    "            # ax.tick_params(axis='x', labelsize= 40 )\n",
    "            ax.tick_params(axis='y', labelsize= 40 )\n",
    "            #shap.summary_plot(shape_values_list, x_train_list1, plot_type=\"bar\",max_display=variable_length)\n",
    "            ax.set_title(\"absolute shapely values \" + name_for_saving+' ('+str(num_runs)+' iters)',fontsize=40)\n",
    "            plt.grid()\n",
    "            fig.tight_layout()\n",
    "\n",
    "\n",
    "            shap_dir = '/Users/huangliting/Desktop/kissing_bugs_stats/shap_importance/'+name_for_saving\n",
    "\n",
    "\n",
    "            if not os.path.exists(shap_dir):\n",
    "                os.makedirs(shap_dir)\n",
    "\n",
    "\n",
    "            store_graph_name = shap_dir+'/'+name_for_saving+'_'+str(num_runs)+'iters.png'\n",
    "            print(store_graph_name)\n",
    "            plt.savefig(shap_dir+'/'+name_for_saving+'_'+str(num_runs)+'iters.png')\n",
    "            plt.clf()\n",
    "\n",
    "            \"\"\"\n",
    "            save prediction\n",
    "            \"\"\"\n",
    "\n",
    "            #obtain average prediction list\n",
    "            testing_group_size_pre = len(final_testing_group['MSP'])\n",
    "            prediction_final_list = np.mean(predict_list.reshape(num_runs, testing_group_size_pre), axis=0)\n",
    "\n",
    "            # initialize data of lists.\n",
    "            final_list_data_f = {'id': id_list,\n",
    "                    'prediction': prediction_final_list}\n",
    "\n",
    "            # Create DataFrame\n",
    "            dataframe_fial_prediction = pd.DataFrame(final_list_data_f)\n",
    "            \"\"\"change_name\"\"\"\n",
    "            #create a directory for storing the trees\n",
    "            pre_dir = '/Users/huangliting/Desktop/kissing_bugs_stats/predictions/'+name_for_saving\n",
    "\n",
    "            #print(pre_dir)\n",
    "\n",
    "            if not os.path.exists(pre_dir):\n",
    "                os.makedirs(pre_dir)\n",
    "\n",
    "\n",
    "            dataframe_fial_prediction.to_csv( pre_dir+'/'+name_for_saving+'_'+str(num_runs)+'iters.csv')\n",
    "            #print(pre_dir+'/'+name_for_saving+'_'+str(num_runs)+'iters.csv')\n",
    "            \"\"\"\n",
    "            write txt\n",
    "            \"\"\"\n",
    "\n",
    "            with open('/Users/huangliting/Desktop/kissing_bugs_stats/txt_results/'+name_for_saving+'_'+str(num_runs)+'iters.txt', 'a') as f:\n",
    "                f.write(name_for_saving+'\\n')\n",
    "                f.write('number of presence points: '+str(number_of_presence)+'\\n')\n",
    "                f.write('cross validation score: '+str(cross_val_score_ave)+'\\n')\n",
    "                f.write('auc: '+str(roc_auc_count_ave)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
